# Effective-strategy-for-LMaas-PaperList
Effective strategy for LMaas: PaperList

我们对Effective strategy for LMaas问题目前相关的进展进行了全面的整理。本列表根据输入缩减、集成方案选择和答案引导对论文进行了分类，并会持续进行更新。
- __新增论文！2023年1月24日：新增论文XXX__
- 2023年1月23日：创建列表


## Input Abstract
### Sentence Reduction
1. The factual inconsistency problem in abstractive text summarization: A survey. Yi-Chong Huang, Xia-Chong Feng,Xiao-Cheng Feng, and Bing Qin. CoRR,2021.[[pdf](https://arxiv.org/pdf/2104.14839v1.pdf)]
2. A comparative survey of text summarization techniques. Patcharapruek Watanangura, Sukit Vanichrudee, On Minteer, Theeranat Sringamdee, Nattapong Thanngam, and Thitirat Siriborvornratanakul. SN Comput. Sci., 2024.[[pdf](https://link.springer.com/article/10.1007/s42979-023-02343-6)]
3. A survey of advanced methods for efficient text summarization. Dinu Antony, Sumit Abhishek, Sujata Singh, Siddu Kodagali, Narayana Darapaneni, Mukesh Rao, Anwesh Reddy Paduri, and Sudha BG. In 13th IEEE Annual Computing and Communication Workshop and Conference, CCWC 2023, Las Vegas, NV, USA, March 8-11, 2023, 2023.[[pdf](https://ieeexplore.ieee.org/document/10099322)]
4. A survey of automatic text summarization: Progress, process and challenges. Muhammad F. Mridha, Aklima Akter Lima, Kamruddin Nur, Sujoy Chandra Das, Mahmud Hasan, and Muhammad Mohsin Kabir. IEEE Access, 2021.[[pdf](https://ieeexplore.ieee.org/document/10099322)]
5. TCRA-LLM: token compression retrieval augmented large language model for inference cost reduction. Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. In Proc. of EMNLP Findings, 2023.[[pdf](https://arxiv.org/pdf/2310.15556.pdf)]
6. Mondrian: Prompt abstraction attack against large language models for cheaper API pricing. Wai Man Si, Michael Backes, and Yang Zhang. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2308.03558.pdf)]
7.Learned token pruning for transformers. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. In Proc. of KDD, 2022.[[pdf](https://arxiv.org/pdf/2308.03558.pdf)]
8. Do all languages cost the same? tokenization in the era of commercial language models. Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith,and Yulia Tsvetkov. In Proc. of EMNLP, 2023[[pdf](https://arxiv.org/pdf/2305.13707.pdf)]
9. Overprompt: Enhancing chatgpt capabilities through an efficient in-context learning approach. Jiazheng Li, Runcong Zhao, Yulan He, and Lin Gui. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2305.14973.pdf)]
### Prompt Optimization
1. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. ACM Comput. Surv., 2023[[pdf](https://dl.acm.org/doi/pdf/10.1145/3560815)]
2. Frugal prompting for dialog models. Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, and Pawan Goyal. In Proc. of EMNLP Findings, 2023.[[pdf](https://arxiv.org/pdf/2305.14919.pdf)]
3. Leancontext: Cost-efficient domain-specific question answering using llms. Md. Adnan Arefeen, Biplob Debnath, and Srimat Chakradhar. CoRR, 2023. [[pdf](https://arxiv.org/pdf/2309.00841.pdf)]
4. Cost-effective testing of a deep learning model through input reduction. Jianyi Zhou, Feng Li, Jinhao Dong, Hongyu Zhang, and Dan Hao. In 31st IEEE International Symposium on Software Reliability Engineering, ISSRE 2020, Coimbra, Portugal, October 12-15, 2020, 2020.[[pdf](https://ieeexplore.ieee.org/document/9251075)]
5. Which tokens to use? investigating token reduction in vi638 sion transformers. Joakim Bruslund Haurum, Sergio Es636 calera, Graham W. Taylor, and Thomas B. Moeslund. In Proc. of ICCV, 2023.[[pdf]([111](https://openaccess.thecvf.com/content/ICCV2023W/NIVT/papers/Haurum_Which_Tokens_to_Use_Investigating_Token_Reduction_in_Vision_Transformers_ICCVW_2023_paper.pdf))][[code](https://vap.aau.dk/tokens/)]
6. Black-box prompt tuning for vision-language model as a service. Lang Yu, Qin Chen, Jiaju Lin, and Liang He. In Proc. of IJCAI, 2023.[[pdf](https://arxiv.org/pdf/2201.03514.pdf)]
7. On improving summarization factual con677 sistency from natural language feedback. Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Halfaker, Dragomir Radev, and Ahmed Has676 san Awadallah. In Proc. of ACL, 2023.[[pdf](https://arxiv.org/pdf/2212.09968.pdf)]
8. Chain of thought prompting elicits knowledge augmentation. Dingjun Wu, Jing Zhang, and Xinmei Huang. In Proc. of ACL Findings, 2023[[pdf](https://arxiv.org/pdf/2201.11903.pdf)]

## Semantic Cache
### Traditional Cache
1. Gptcache: An open-source semantic cache for llm applications enabling faster answers and cost savings. Fu Bang. 2023. [[pdf](https://www.researchgate.net/publication/376404523_GPTCache_An_Open-Source_Semantic_Cache_for_LLM_Applications_Enabling_Faster_Answers_and_Cost_Savings)][[code](https://github.com/zilliztech/GPTCache)]
2. On optimal caching and model multiplexing for large model inference. Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark W. Barrett, Michael I. Jordan, and Jiantao Jiao. CoRR, 2023[[pdf](https://arxiv.org/pdf/2306.02003.pdf)]
3. A survey on response selection for retrieval-based dialogues. Chongyang Tao, Jiazhan Feng, Rui Yan, Wei Wu, and Daxin Jiang. In Proc. of IJCAI, 2021.[[pdf](https://www.ijcai.org/proceedings/2021/0627.pdf)]
4. Service caching and computation reuse strategies at the edge: A survey. Carlos Barrios and Mohan Kumar. ACM Comput. Surv., 2024. [[pdf](https://dl.acm.org/doi/10.1145/3609504)]
### Neural Caching
1. Cache & distil: Otimising API calls to large language models. Guillem Ram´ırez, Matthias Lindemann, Alexandra Birch, and Ivan Titov. CoRR, 2023.[[pdf](Cache & Distil: Optimising API Calls to Large Language Models)]
2. Llms for test input generation for semantic caches. Zafaryab Rasool, Scott Barnett, David Willie, Stefanus Kurniawan, Sherwin Balugo, Srikanth Thudumu, and Mohamed Abdelrazek, 2024. [[pdf](https://arxiv.org/pdf/2401.08138.pdf)]
3. A survey on response selection for retrieval-based dialogues. Chongyang Tao, Jiazhan Feng, Rui Yan, Wei Wu, and Daxin Jiang. In Proc. of IJCAI, 2021.[[pdf](https://www.ijcai.org/proceedings/2021/0627.pdf)]

## Solution Design
### Scoring function
1. Cache & distil: Optimising API calls to large language models. Guillem Ram´ırez, Matthias Lindemann, Alexandra Birch, and Ivan Titov. CoRR, 2023. [[pdf](https://arxiv.org/pdf/2310.13561.pdf#page=11&zoom=100,401,805)]
2. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2310.03094.pdf)]
3. On optimal caching and model multiplexing for large model inference. Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark W. Barrett, Michael I. Jordan, and Jiantao Jiao. CoRR, 2023[[pdf](https://arxiv.org/pdf/2306.02003.pdf)]
4. Routing to the expert: Efficient reward-guided ensemble of large language models. Keming Lu, Hongyi Yuan, Runji Lin, Jun680 yang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2311.08692.pdf)]
5. Frugalgpt: How to use large language models while reducing cost and improving performance. Lingjiao Chen, Matei Zaharia, and James Zou. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2305.05176.pdf)]
6. Fly-swat or cannon? cost-effective language model choice via meta-modeling. Marija Sakota, Maxime Peyrard, and Robert West. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2308.06077.pdf)][[code](https://github.com/epfl-dlab/forc)]
7. Large language model routing with benchmark datasets. Tal Shnitzer, Anthony Ou, M´ırian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, and Mikhail Yurochkin. CoRR, 2023. [[pdf](https://arxiv.org/pdf/2309.15789.pdf)]
8. EcoAssistant: Using LLM assistant more affordably and accurately. Jieyu Zhang, Ranjay Krishna, Ahmed Hassan Awadallah, and Chi Wang. CoRR, 2023.[[pdf]((https://arxiv.org/pdf/2310.03046.pdf))]
9. Hybrid LLM: Cost-efficient and quality-aware query routing.Anonymous. In The Twelfth International Conference on Learning Representations, 2024.[[pdf](https://openreview.net/pdf?id=02f3mUtqnM)]
10. Automix: Automatically mixing language models. Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, and Manaal Faruqui. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2310.12963.pdf)]
### LLM Router
1. Frugalgpt: How to use large language models while reducing cost and improving performance. Lingjiao Chen, Matei Zaharia, and James Zou. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2310.13561.pdf#page=11&zoom=100,401,805)]
2. Cache & distil: Optimising API calls to large language models. Guillem Ram´ırez, Matthias Lindemann, Alexandra Birch, and Ivan Titov. CoRR, 2023. [[pdf](https://arxiv.org/pdf/2310.13561.pdf#page=11&zoom=100,401,805)]
3. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2310.03094.pdf)]
4. EcoAssistant: Using LLM assistant more affordably and accurately. Jieyu Zhang, Ranjay Krishna, Ahmed Hassan Awadallah, and Chi Wang. CoRR, 2023.[[pdf]((https://arxiv.org/pdf/2310.03046.pdf))]
5. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, and Bill YuchenLin. In Proc. of ACL,2023.[[pdf](https://arxiv.org/pdf/2306.02561.pdf)]
6. Fly-swat or cannon? cost-effective language model choice via meta-modeling. Marija Sakota, Maxime Peyrard, and Robert West. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2308.06077.pdf)]
7. Routing to the expert: Efficient reward-guided ensemble of large language models. Keming Lu, Hongyi Yuan, Runji Lin, Jun yang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2311.08692.pdf)]
8. Automix: Automatically mixing language models. Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, and Manaal Faruqui. CoRR, 2023.[[pdf](https://arxiv.org/pdf/2310.12963.pdf)]
9. Service selection using multi-criteria decision making: A comprehensive overview. Mehdi Hosseinzadeh, Hawkar Kamaran Hama, Marwan Yassin Ghafour, Mohammad Masdari, Omed Hassan Ahmed, and Hemn Khezri. J. Netw. Syst. Manag., 2020[[pdf](https://arxiv.org/pdf/2310.12963.pdf)]
10. A survey for service selection approaches in dynamic environments. Lindelweyizizwe Manqele, Mqhele E. Dlodlo, Louis Coetzee, and George Sibiya. In IEEE AFRICON 2017, Cape Town, South Africa, September 18-20, 2017, 2017. [[pdf](https://ieeexplore.ieee.org/document/8095627)]
## Output Enhancement
1. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. ACM Comput. Surv., 2023.[[pdf](https://dl.acm.org/doi/pdf/10.1145/3560815)]
2. Efficient online ML API selection for multi-label classification tasks. Lingjiao Chen, Matei Zaharia, and James Zou. In Proc. of ICML, 2022.[[pdf](https://arxiv.org/abs/2102.09127)] 
